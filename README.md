## Document
### Recommended Environment

- [x] torch 1.12.0
- [x] torchvision 0.13.0
- [x] numpy 1.24.5
- [x] nncf 2.13.0
- [x] openvino 2024.4.0
- [x] openvino-dev 2024.4.0
- [x] openvino-telemetry 2025.2.0
- [x] onnx 1.16.1


### Train
You can choose DEYOLO's n/s/m/l/x model in [DEYOLO.yaml](./ultralytics/models/v8/DEYOLO.yaml)

```python
from ultralytics import YOLO

# Load a model
model = YOLO("ultralytics/models/v8/DEYOLO.yaml").load("yolov8n.pt")

# Train the model
train_results = model.train(
    data="M3FD.yaml",  # path to dataset YAML
    epochs=100,  # number of training epochs
    imgsz=640,  # training image size
    device="cpu",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu
)
```

### Predict

```python
from ultralytics import YOLO

# Load a model
model = YOLO("DEYOLOn.pt") # trained weights

# Perform object detection on RGB and IR image
model.predict([["ultralytics/assets/vi_1.png", "ultralytics/assets/ir_1.png"], # corresponding image pair
              ["ultralytics/assets/vi_2.png", "ultralytics/assets/ir_2.png"]], 
              save=True, imgsz=320, conf=0.5)
```

### NNCF quantize
åœ¨è¿›è¡Œnncfé‡åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå°†æˆ‘ä»¬çš„ptæ¨¡å‹è½¬æˆonnxæ¨¡å‹ï¼Œç›´æ¥å…‹éš†[DEYOLO](https://github.com/chips96/DEYOLO/issues/24?reload=1)ä»“åº“ç„¶åå¯¼å‡ºonnxæ—¶ä¼šæŠ¥é”™ï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼Œ
è¿›å…¥åˆ°yolo/engine/exporter.pyä¸­ï¼Œæ‰¾åˆ°export_onnxæ¨¡å—ï¼Œç„¶åå°†å…¶æŒ‰ç…§å¦‚ä¸‹ä¿®æ”¹ï¼š
```python
torch.onnx.export(
    self.model.cpu() if dynamic else self.model,  # --dynamic only compatible with cpu
    (self.im.cpu() if dynamic else self.im, self.im2.cpu() if dynamic else self.im2),
    f,
    verbose=False,
    opset_version=11,
    do_constant_folding=True,  # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False
    input_names=['images','images1'],
    output_names=output_names,
    dynamic_axes=dynamic or None)
```

* PTQ (implicit)ï¼šä½¿ç”¨PTQ_NNCF_implicit(v2).pyè„šæœ¬å³å¯å®Œæˆint8é‡åŒ–ï¼Œéœ€è¦å‡†å¤‡æ ¡å‡†æ•°æ®é›†ï¼Œä½¿ç”¨çš„æ˜¯ultralyticsä¸­çš„dataloaderã€‚
```python
def create_data_source():
    from ultralytics.yolo.engine.trainer import BaseTrainer
    from ultralytics.yolo.v8.detect.train import DetectionTrainer
    from ultralytics.yolo.utils import DEFAULT_CFG

    cfg = DEFAULT_CFG
    model = "ultralytics/models/v8/DEYOLO.yaml"
    data = cfg.data or "ultralytics/yolo/cfg/M3FD.yaml"  # or yolo.ClassificationDataset("mnist")
    device = cfg.device if cfg.device is not None else ''

    args = dict(model=model, data=data, device=device)
    trainer = DetectionTrainer(overrides=DEFAULT_CFG)
    trainer.setup_model()
    vis_dir = r"D:\company_Tenda\35.DEYOLO\DEYOLO\dataset\images\vis_train"
    inf_dir = r"D:\company_Tenda\35.DEYOLO\DEYOLO\dataset\images\ir_train"
    dataloader = trainer.get_dataloader(
        dataset_path = vis_dir,
        dataset_path2 = inf_dir,
        batch_size = 1,
        rank = -1,
        mode = 'train'  # æˆ– 'val'
    )
    return dataloader


def transform_fn(batch):
    batch_input = dict()
    # ç½‘ç»œçš„è¾“å…¥èŠ‚ç‚¹åç§°æ˜¯imageså’Œimages1
    batch_input['images'] = batch['img'].to(torch.device("cpu"), non_blocking=True).float() / 255
    batch_input['images1'] = batch['img2'].to(torch.device("cpu"), non_blocking=True).float() / 255
    return batch_input
```

æˆ–è€…ä½¿ç”¨è‡ªå®šä¹‰çš„dataloaderç±»ï¼š
```python
class CalibrationDataset:
    def __init__(self, data_path_vis=DATA_PATH_VIS, data_path_ir = DATA_PATH_IR, num_samples=300):

        self.data_path_vis = Path(data_path_vis)
        self.data_path_ir = Path(data_path_ir)
        self.num_samples = num_samples

        # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®å›¾åƒå¯ç”¨ï¼Œå¦‚æœè¦æ±‚æ¨¡å‹å‡†ç¡®ï¼Œ
        if self.data_path_vis.exists() and len(list(self.data_path_vis.glob("*.*"))) > 0 and self.data_path_ir.exists() and len(list(self.data_path_ir.glob("*.*"))) > 0:
            self.image_files_vis = list(self.data_path_vis.glob("*.*"))[:num_samples]
            self.image_files_ir = list(self.data_path_ir.glob("*.*"))[:num_samples]
            self.use_real_images = True
            print(f"ä½¿ç”¨çœŸå®æ ¡å‡†å›¾åƒ: {len(self.image_files_vis)}å¼ ")
        else:
            self.use_real_images = False
            print(f"ä½¿ç”¨éšæœºç”Ÿæˆæ•°æ®: {num_samples}å¼ ")

    def __getitem__(self, index):
        if self.use_real_images:
            # ä»çœŸå®å›¾åƒåŠ è½½
            img_path_vis = self.image_files_vis[index % len(self.image_files_vis)]
            img_vis = cv2.imread(str(img_path_vis))
            img_vis = cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB)

            img_path_ir = self.image_files_ir[index % len(self.image_files_ir)]
            img_ir = cv2.imread(str(img_path_ir))
            img_ir = cv2.cvtColor(img_ir, cv2.COLOR_BGR2RGB)
        else:
            # ç”Ÿæˆéšæœºå›¾åƒ
            img_vis = np.random.randint(0, 256, (640, 640, 3), dtype=np.uint8)
            img_ir = np.random.randint(0, 256, (640, 640, 3), dtype=np.uint8)

        # é¢„å¤„ç†
        img_vis = cv2.resize(img_vis, (640, 640))
        img_vis = img_vis.transpose(2, 0, 1)  # HWC to CHW
        img_vis = img_vis[np.newaxis, ...]  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦

        img_ir = cv2.resize(img_ir, (640, 640))
        img_ir = img_ir.transpose(2, 0, 1)  # HWC to CHW
        img_ir = img_ir[np.newaxis, ...]  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        return {"images": img_vis.astype(np.float32) / 255.0, "images1": img_ir.astype(np.float32) / 255.0}
```

* PTQ (explicit): ä½¿ç”¨PTQ_NNCF_explicit.pyè„šæœ¬å³å¯å®Œæˆint8é‡åŒ–ï¼Œæ’å…¥äº†QDQèŠ‚ç‚¹è¿›è¡Œæ˜¾ç¤ºé‡åŒ–ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„modelç›´æ¥è¿›è¡Œæ¨ç†æ—¶
ä¼šæœ‰ä¸€ä¸ªä¸¤ä¸ªè¾“å…¥çš„æŠ¥é”™ï¼Œç”±äºultralyticsä½¿ç”¨çš„æ˜¯nn.Sequential()æ¥æ„å»ºçš„ç½‘ç»œï¼Œæ˜¯æ— æ³•ç›´æ¥æ¥å—ä¸¤ä¸ªè¾“å…¥çš„ï¼Œå› æ­¤æˆ‘åœ¨[DEYOLO_net.py](./DEYOLO_net.py)ä¸­å®ç°äº†å¯ä»¥ç›´æ¥æ¥æ”¶ä¸¤ä¸ªè¾“å…¥çš„æ¨¡å‹æ¡†æ¶ã€‚

```python
 # dont workğŸ¤¡ğŸ¤¡
DEYOLO_ = YOLO(r"your model yaml path")
model = DEYOLO_.model.model
...
_ = model(input1, input2)

# work ğŸ˜ğŸ˜
from DEYOLO_net import DEYOLO 
model = DEYOLO()
...
_ = model(input1, input2)
```

* QAT (æ•¬è¯·æœŸå¾…)


## Dataset
Like [M3FD.yaml](./ultralytics/yolo/cfg/M3FD.yaml) and [LLVIP.yaml](./ultralytics/yolo/cfg/LLVIP.yaml) You can use your own dataset.

<details open>
  <summary><b>File structure</b></summary>

```
Your dataset
â”œâ”€â”€ ...
â”œâ”€â”€ images
|   â”œâ”€â”€ vis_train
|   |   â”œâ”€â”€ 1.jpg
|   |   â”œâ”€â”€ 2.jpg
|   |   â””â”€â”€ ...
|   â”œâ”€â”€ vis_val
|   |   â”œâ”€â”€ 1.jpg
|   |   â”œâ”€â”€ 2.jpg
|   |   â””â”€â”€ ...
|   â”œâ”€â”€ Ir_train
|   |   â”œâ”€â”€ 100.jpg
|   |   â”œâ”€â”€ 101.jpg
|   |   â””â”€â”€ ...
|   â”œâ”€â”€ Ir_val 
|   |   â”œâ”€â”€ 100.jpg
|   |   â”œâ”€â”€ 101.jpg
|   |   â””â”€â”€ ...
â””â”€â”€ labels
    â”œâ”€â”€ vis_train
    |   â”œâ”€â”€ 1.txt
    |   â”œâ”€â”€ 2.txt
    |   â””â”€â”€ ...
    â””â”€â”€ vis_val
        â”œâ”€â”€ 100.txt
        â”œâ”€â”€ 101.txt
        â””â”€â”€ ...
```

</details>

You can download the dataset using the following link:
- [M3FD](https://github.com/JinyuanLiu-CV/TarDAL)
- [LLVIP](https://github.com/bupt-ai-cz/LLVIP)

## Pipeline
### The framework
<div align="center">
  <img src="imgs/network.png" alt="network" width="800" />
</div>

 We incorporate dual-context collaborative enhancement modules (DECA and DEPA) within the feature extraction
 streams dedicated to each detection head in order to refine the single-modality features
 and fuse multi-modality representations. Concurrently, the Bi-direction Decoupled Focus is inserted in the early layers of the YOLOv8 backbone to expand the networkâ€™s
 receptive fields.

### DECA and DEPA
<div align="center">
  <img src="imgs/DECA-DEPA.png" alt="DECA-DEPA" width="800" />
</div>

DECA enhances the cross-modal fusion results by leveraging dependencies between
channels within each modality and outcomes are then used to reinforce the original
single-modal features, highlighting more discriminative channels.  

DEPA is
able to learn dependency structures within and across modalities to produce enhanced
multi-modal representations with stronger positional awareness.

### Bi-direction Decoupled Focus
<div align="center">
  <img src="imgs/bi-focus.png" alt="bi-focus" width="400">
</div>

We divide the pixels into two groups for convolution.
Each group focuses on the adjacent and remote pixels at the same time.
Finally, we concatenate the original feature map in the channel dimension and
make it go through a depth-wise convolution layer.

## Visual comparison
<div align="center">
  <img src="imgs/comparison.png" alt="comparison" width="800" />
</div>

## Main Results
<div align="center">
  <img src="imgs/map.png" alt="map" width="600" />
</div>

 The mAP<sub>50</sub> and mAP<sub>50âˆ’95</sub> of every category in M<sup>3</sup>FD dataset demonstrate the superiority of our method.
 
 Trained Weightsï¼š
 - [M3FD](https://pan.baidu.com/s/1fZx0UjFcyTfRqZfgKRSZgA?pwd=3016)
 - [LLVIP](https://pan.baidu.com/s/1rw5qdCbvLTlcREoAsNMRXw?pwd=3016)
 - [Kaist](https://pan.baidu.com/s/1b-NO4lteXK-TwSTBrGuXsQ?pwd=3016)

